Sure! Here's a well-structured and clear `README.md` file for your annotation tool, incorporating both usage instructions and the annotation guideline:

---

# ðŸ§  Knowledge Graph Annotation Tool

This tool is designed to help annotators evaluate knowledge graph (KG) generations from two models, based on their adherence to structure, preservation of meaning, and semantic quality.

The interface is built with [Gradio](https://www.gradio.app/) and allows step-by-step annotation of model outputs, including saving progress and resuming where you left off.

---

## ðŸš€ How to Run the Annotation Tool

1. **Install dependencies** (if not already installed):
   ```bash
   pip install gradio pandas
   ```

2. **Prepare your CSV file** with the following columns:
   - `input_text`: The original context text.
   - `generated_text_base`: The KG generated by the base model.
   - `generated_text_lora`: The KG generated by the fine-tuned model.

3. **Run the script**:
   ```bash
   python annotation_tool.py
   ```

4. **Enter file name** and **annotator name** when prompted:
   - Example:
     ```
     Enter your file name: my_dataset.csv
     Enter your annotator name: Alice
     ```

5. The Gradio interface will launch in your browser.

---

## ðŸ–±ï¸ Interface Features

- **View Context:** See the original text input.
- **Compare Models:** View the outputs from both models side-by-side.
- **Rate Each Model:** Select a rating from A to E for each model output.
- **Choose Preferred Output:** Indicate which model's generation you prefer overall.
- **Navigate:** Submit or go back to the previous example.
- **Progress is automatically saved** in `progress.txt` and the same CSV file.

---

## ðŸ“ Annotation Guidelines

Each generated KG should be evaluated based on three criteria:

### 1. Template Adherence
- Should include **exactly 5 triples**, no more or less.
- Each triple follows the format:  
  ```
  (Subject -> Predicate -> Object)
  ```
- No extra or unrelated text outside the triple list.

### 2. Meaning Preservation
- Triples should accurately reflect **important facts** from the original context.
- Key relationships and entities must be **faithfully extracted**.

### 3. Structural & Semantic Quality
- Triples must be **well-formed** and **clear**.
- Relations should be **logical, factual, and informative**.

---

## ðŸ“Š Rating Scale

| Grade | Description |
|-------|-------------|
| **A** | Perfect output. All 5 triples are accurate, clean, and meaningful. |
| **B** | Minor issues (format or detail), but high quality overall. |
| **C** | Acceptable but includes minor structural or semantic problems. |
| **D** | Several issues. Triple quality or relevance is poor. |
| **E** | Unusable. Format broken or meaning completely lost. |

---

## âœ… Rating Tips

- **Start by checking**: Are there 5 properly formatted triples?
- **Compare with context**: Do the triples capture key facts from the text?
- **Evaluate relation clarity**: Are the subject-predicate-object relationships logical?

---

## ðŸ“ Output Columns

The script will save your annotations into the original CSV with the following columns added:
- `rating_model_kg_mian`
- `rating_model_kg_lora`
- `preferred_kg`
- `annotator`
- `annotation_time` (seconds spent per example)

---

## ðŸ›  Example

```csv
input_text,generated_text_base,generated_text_lora,...
"Alice was born in Paris and works at NASA.",...
"(Alice -> born_in -> Paris)\n(Alice -> works_at -> NASA)...",...
```
